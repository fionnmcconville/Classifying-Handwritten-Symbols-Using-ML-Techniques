column_names = c('label','index', 'nr_pix', 'height', 'width', 'tallness',
'rows_with_1','cols_with_1', 'rows_with_5_plus', 'cols_with_5_plus',
'one_neighbour', 'three_or_more_neighbours', 'no_neighbours_below',
'no_neighbours_above', 'no_neighbours_before', 'no_neighbours_after',
'nr_regions', 'nr_eyes', 'r5_c5')
column_names
features <- read.csv("../features/40156103_features.csv", header = FALSE, col.names = column_names)
features["Category"] <- NA
features$Category[1:56] <- "Digits"
features$Category[57:112] <- "Letters"
features$Category[113:168] <- "Math"
features$Category <- as.factor(features$Category)
features$label[features$label==12] <- '2'
features$label[features$label==13] <- '3'
features$label[features$label==14] <- '4'
features$label[features$label==11] <- '1'
features$label[features$label==15] <- '5'
features$label[features$label==16] <- '6'
features$label[features$label==17] <- '7'
features$label[features$label==21] <- 'a'
features$label[features$label==22] <- 'b'
features$label[features$label==23] <- 'c'
features$label[features$label==24] <- 'd'
features$label[features$label==26] <- 'f'
features$label[features$label==27] <- 'g'
features$label[features$label==31] <- '<'
features$label[features$label==32] <- '>'
features$label[features$label==33] <- '='
features$label[features$label==34] <- '<='
features$label[features$label==35] <- '>='
features$label[features$label==36] <- 'not='
features$label[features$label==25] <- 'e'
features$label[features$label==37] <- 'approx'
features$label <- as.factor(features$label)
lettersdigitsdf <- features[1:112,]
# According to my statistical analysis in section 3.8 in assignment 1 the most significant feature is no_neighbours_after
library(ggplot2)
library(caret)
plt <- ggplot(lettersdigitsdf, aes(x=no_neighbours_after, fill=as.factor(Category))) +
geom_histogram(binwidth = 2, alpha = .5, position = "dodge") +
labs(title="Histogram of no_neighbours_after feature between Digits and Letters",
x="Value of no_neighbours_after feature", y = "Count", fill = "Category")
plt
lettersdigitsdf$dummy.category <- 0
lettersdigitsdf$dummy.category[lettersdigitsdf$Category == 'Letters'] <- 1
lettersdigitsdf$dummy.category
glmfit<-glm(dummy.category ~ no_neighbours_after, data = lettersdigitsdf, family = 'binomial')
summary(glmfit)
#plotting a fitted curve and lettersdigitsdf data
x.range = range(lettersdigitsdf[["no_neighbours_after"]])
x.range
x.values = seq(x.range[1],x.range[2],length.out=1000)
x.values
fitted.curve <- data.frame(no_neighbours_after = x.values)
fitted.curve[["dummy.category"]] = predict(glmfit, fitted.curve, type="response")
# Plot the training data and the fitted  curve:
plt <-ggplot(lettersdigitsdf, aes(x=no_neighbours_after, y=dummy.category)) +
geom_point(aes(colour = factor(dummy.category)),
show.legend = T, position="dodge")+
geom_line(data=fitted.curve, colour="orange", size=1)
plt
# Plot the training data and the fitted  curve:
plt <-ggplot(lettersdigitsdf, aes(x=no_neighbours_after, y=dummy.category)) +
geom_point(aes(colour = factor(dummy.category)),
show.legend = T, position="dodge")+
geom_line(data=fitted.curve, colour="orange", size=1) +
labs(title="Logistic regression with no_neighbours_after feature",
x="Amount of no_neighbours_after", y = "Probability class is Letter")
plt
# Plot the training data and the fitted  curve:
plt <-ggplot(lettersdigitsdf, aes(x=no_neighbours_after, y=dummy.category)) +
geom_point(aes(colour = factor(dummy.category)),
show.legend = T, position="dodge")+
geom_line(data=fitted.curve, colour="orange", size=1) +
labs(title="Logistic regression with no_neighbours_after feature",
x="Amount of no_neighbours_after", y = "Probability class is Letter", color = "Category") +
scale_color_manual(labels = c("Digits", "Letters"))
plt
# Plot the training data and the fitted  curve:
plt <-ggplot(lettersdigitsdf, aes(x=no_neighbours_after, y=dummy.category)) +
geom_point(aes(colour = factor(dummy.category)),
show.legend = T, position="dodge")+
geom_line(data=fitted.curve, colour="orange", size=1) +
labs(title="Logistic regression with no_neighbours_after feature",
x="Amount of no_neighbours_after", y = "Probability class is Letter", color = "Category") +
scale_color_manual(labels = c("Digits", "Letters"), values = c("blue", "red"))
plt
# Plot the training data and the fitted  curve:
plt <-ggplot(lettersdigitsdf, aes(x=no_neighbours_after, y=dummy.category)) +
geom_point(aes(colour = factor(dummy.category)),
show.legend = T, position="dodge")+
geom_line(data=fitted.curve, colour="orange", size=1) +
labs(title="Logistic regression with no_neighbours_after feature",
x="Amount of no_neighbours_after", y = "Probability Class is Letter", color = "Category") +
scale_color_manual(labels = c("Digits", "Letters"), values = c("blue", "red"))
plt
ggsave('letters_digits_fitted_curve.png',scale=0.7,dpi=400)
getwd()
x.values = lettersdigitsdf[["no_neighbours_after"]]
lettersdigitsdf[["predicted_val"]] = predict(glmfit, lettersdigitsdf, type="response")
lettersdigitsdf[["predicted_class"]] = 0
lettersdigitsdf[["predicted_class"]][lettersdigitsdf[["predicted_val"]] > 0.5] = 1
correct_items = lettersdigitsdf[["predicted_class"]] == lettersdigitsdf[["dummy.category"]]
# proportion correct:
nrow(lettersdigitsdf[correct_items,])/nrow(lettersdigitsdf)
# proportion incorrect:
nrow(lettersdigitsdf[!correct_items,])/nrow(lettersdigitsdf)
lettersdigitsdf <- features[1:112,]
lettersdigitsdf$dummy.category <- 0
lettersdigitsdf$dummy.category[lettersdigitsdf$Category == 'Letters'] <- 1
vars <- paste(names(lettersdigitsdf)[3:10],sep="")
firstfeatures <- paste("dummy.category ~", paste(vars, collapse="+"))
firstfeatures <- as.formula(firstfeatures)
glmfit1<-glm(firstfeatures, data = lettersdigitsdf, family = 'binomial')
summary(glmfit1)
lettersdigitsdf[["predicted_val"]] = predict(glmfit1, lettersdigitsdf, type="response")
lettersdigitsdf[["predicted_class"]] = 0
lettersdigitsdf[["predicted_class"]][lettersdigitsdf[["predicted_val"]] > 0.5] = 1
correct_items = lettersdigitsdf[["predicted_class"]] == lettersdigitsdf[["dummy.category"]]
# proportion correct:
nrow(lettersdigitsdf[correct_items,])/nrow(lettersdigitsdf)
# proportion incorrect:
nrow(lettersdigitsdf[!correct_items,])/nrow(lettersdigitsdf)
plt <- ggplot(lettersdigitsdf, aes(x=no_neighbours_after, fill=as.factor(Category))) +
geom_histogram(binwidth = 2, alpha = .5, position = "identity") +
labs(title="Histogram of no_neighbours_after feature between Digits and Letters",
x="Value of no_neighbours_after feature", y = "Count", fill = "Category")
plt
plt <- ggplot(lettersdigitsdf, aes(x=no_neighbours_after, fill=as.factor(Category))) +
geom_histogram(binwidth = 1, alpha = .5, position = "identity") +
labs(title="Histogram of no_neighbours_after feature between Digits and Letters",
x="Value of no_neighbours_after feature", y = "Count", fill = "Category")
plt
plt <- ggplot(lettersdigitsdf, aes(x=no_neighbours_after, fill=as.factor(Category))) +
geom_histogram(binwidth = 2, alpha = .5, position = "identity") +
labs(title="Histogram of no_neighbours_after feature between Digits and Letters",
x="Value of no_neighbours_after feature", y = "Count", fill = "Category")
plt
ggsave("Histogram of no_neighbours_after feature between Digits and Letters.png")
glmfit1<-glm(firstfeatures, data = lettersdigitsdf, family = 'binomial')
summary(glmfit1)
lettersdigitsdf[["predicted_val"]] = predict(glmfit1, lettersdigitsdf, type="response")
lettersdigitsdf[["predicted_class"]] = 0
lettersdigitsdf[["predicted_class"]][lettersdigitsdf[["predicted_val"]] > 0.5] = 1
correct_items = lettersdigitsdf[["predicted_class"]] == lettersdigitsdf[["dummy.category"]]
# proportion correct:
nrow(lettersdigitsdf[correct_items,])/nrow(lettersdigitsdf)
# proportion incorrect:
nrow(lettersdigitsdf[!correct_items,])/nrow(lettersdigitsdf)
confusionMatrix(data=as.factor(lettersdigitsdf[["predicted_class"]]), as.factor(lettersdigitsdf$dummy.category))
1 - pbinom(78, 112, 0.5) # P value returned is very small, so the model is significant
1 - pbinom(68, 112, 0.5) # P value returned is very small, so the model is significant
cmatrix <- confusionMatrix(data=as.factor(pred(0.5)), as.factor(lettersdigitsdf$Category))
justtable <- cmatrix$table
cmatrix
set.seed(3060)
# #Second method of k fold cross validation
train_control1 <- trainControl(method = "cv", number = 7, savePredictions = TRUE)
#?trainControl
model2 <- train(dummy.category ~ no_neighbours_after,
data = lettersdigitsdf,
trControl = train_control1,
method = "glm",
family=binomial)
summary(model2)
predictions = predict(model2, lettersdigitsdf)
pred <- function(t) ifelse(predictions > t, "Letters","Digits")
#function makes prediction model binary; if a p value > 0.5 then prediction is that symbol is a letter, else it's a digit
pred(0.5)
confusionMatrix(data=as.factor(pred(0.5)), as.factor(lettersdigitsdf$Category))
lettersdigitsdf <- features[1:112,]
lettersdigitsdf$dummy.category <- 0
lettersdigitsdf$dummy.category[lettersdigitsdf$Category == 'Letters'] <- 1
lettersdigitsdf$dummy.category
lettersdigitsdf <- features[1:112,]
lettersdigitsdf$dummy.category <- 0
lettersdigitsdf$dummy.category[lettersdigitsdf$Category == 'Letters'] <- 1
vars <- paste(names(lettersdigitsdf)[3:10],sep="")
firstfeatures <- paste("dummy.category ~", paste(vars, collapse="+"))
firstfeatures <- as.formula(firstfeatures)
glmfit1<-glm(firstfeatures, data = lettersdigitsdf, family = 'binomial')
summary(glmfit1)
lettersdigitsdf[["predicted_val"]] = predict(glmfit1, lettersdigitsdf, type="response")
lettersdigitsdf[["predicted_class"]] = 0
lettersdigitsdf[["predicted_class"]][lettersdigitsdf[["predicted_val"]] > 0.5] = 1
correct_items = lettersdigitsdf[["predicted_class"]] == lettersdigitsdf[["dummy.category"]]
# proportion correct:
nrow(lettersdigitsdf[correct_items,])/nrow(lettersdigitsdf)
# proportion incorrect:
nrow(lettersdigitsdf[!correct_items,])/nrow(lettersdigitsdf)
set.seed(3060)
# #Second method of k fold cross validation
train_control1 <- trainControl(method = "cv", number = 7, savePredictions = TRUE)
#?trainControl
model2 <- train(dummy.category ~ no_neighbours_after,
data = lettersdigitsdf,
trControl = train_control1,
method = "glm",
family=binomial)
summary(model2)
predictions = predict(model2, lettersdigitsdf)
pred <- function(t) ifelse(predictions > t, "Letters","Digits")
#function makes prediction model binary; if a p value > 0.5 then prediction is that symbol is a letter, else it's a digit
pred(0.5)
confusionMatrix(data=as.factor(pred(0.5)), as.factor(lettersdigitsdf$Category))
View(lettersdigitsdf)
View(lettersdigitsdf)
confusionMatrix(data=as.factor(pred(0.5)), as.factor(lettersdigitsdf$Category))
lettersdigitsdf <- features[1:112,]
lettersdigitsdf$dummy.category <- 0
lettersdigitsdf$dummy.category[lettersdigitsdf$Category == 'Letters'] <- 1
str(lettersdigitsdf)
str(lettersdigitsdf)
# #Second method of k fold cross validation
train_control1 <- trainControl(method = "cv", number = 7, savePredictions = TRUE)
#?trainControl
model2 <- train(dummy.category ~ no_neighbours_after,
data = lettersdigitsdf,
trControl = train_control1,
method = "glm",
family=binomial)
summary(model2)
predictions = predict(model2, lettersdigitsdf)
pred <- function(t) ifelse(predictions > t, "Letters","Digits")
#function makes prediction model binary; if a p value > 0.5 then prediction is that symbol is a letter, else it's a digit
pred(0.5)
confusionMatrix(data=as.factor(pred(0.5)), as.factor(lettersdigitsdf$Category))
confusionMatrix(data=as.factor(pred(0.5)), as.factor(lettersdigitsdf$dummy.category))
predictions = predict(model2, lettersdigitsdf)
pred <- function(t) ifelse(predictions > t, 0,1)
#function makes prediction model binary; if a p value > 0.5 then prediction is that symbol is a letter, else it's a digit
pred(0.5)
confusionMatrix(data=as.factor(pred(0.5)), as.factor(lettersdigitsdf$dummy.category))
# 7 fold CV for first 8 features
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
set.seed(3060)
train_control3 <- trainControl(method = "cv", number = 7, savePredictions = TRUE)
#?trainControl
model3 <- train(firstfeatures,
data = lettersdigitsdf,
trControl = train_control3,
method = "glm",
family=binomial)
summary(model3)
model3$pred
predictions1 = predict(model3, lettersdigitsdf)
pred <- function(t) ifelse(predictions1 > t, "Letters","Digits")
pred(0.5)
confusionMatrix(data=as.factor(pred(0.5)), as.factor(lettersdigitsdf$Category))
# #Second method of k fold cross validation
train_control1 <- trainControl(method = "cv", number = 7, savePredictions = TRUE)
#?trainControl
model2 <- train(dummy.category ~ no_neighbours_after,
data = lettersdigitsdf,
trControl = train_control1,
method = "glm",
family=binomial)
summary(model2)
predictions = predict(model2, lettersdigitsdf)
pred <- function(t) ifelse(predictions > t, "Letters","Digits")
#function makes prediction model binary; if a p value > 0.5 then prediction is that symbol is a letter, else it's a digit
pred(0.5)
confusionMatrix(data=as.factor(pred(0.5)), as.factor(lettersdigitsdf$Category))
cmatrix <- confusionMatrix(data=as.factor(pred(0.5)), as.factor(lettersdigitsdf$Category))
justtable <- cmatrix$table
cmatrix
justtable <- cmatrix$table
cmatrix
cmatrix$table
justtable
# 7 fold CV for first 8 features
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
set.seed(3060)
train_control3 <- trainControl(method = "cv", number = 7, savePredictions = TRUE)
#?trainControl
model3 <- train(firstfeatures,
data = lettersdigitsdf,
trControl = train_control3,
method = "glm",
family=binomial)
summary(model3)
model3$pred
predictions1 = predict(model3, lettersdigitsdf)
pred1 <- function(t) ifelse(predictions1 > t, "Letters","Digits")
pred1(0.5)
confusionMatrix(data=as.factor(pred1(0.5)), as.factor(lettersdigitsdf$Category))
cmatrix1 <- confusionMatrix(data=as.factor(pred1(0.5)), as.factor(lettersdigitsdf$Category))
justtable1 <- cmatrix1$table
justtable1 # Confusion matrix for best feature classifier
vars <- paste(names(trainfeatures)[3:12],sep="")
tenfeatures <- paste("label ~", paste(vars, collapse="+"))
tenfeatures <- as.formula(tenfeatures)
tenfeatures
ks <- c(1,3,5,7,9,11,15,19,23,31)
library(caret)
column_names1 <- c('label','index', 'nr_pix', 'height', 'width', 'tallness',
'rows_with_1','cols_with_1', 'rows_with_5_plus', 'cols_with_5_plus',
'one_neighbour', 'three_or_more_neighbours', 'no_neighbours_below',
'no_neighbours_above', 'no_neighbours_before', 'no_neighbours_after',
'nr_regions', 'nr_eyes', 'r5_c5', 'extra', 'extra1', 'extra2')
trainfeatures <- read.csv("../features/trainingfeatures.csv", header = FALSE, col.names = column_names1)
trainfeatures <- trainfeatures[,1:19 ] # Don't need 3 custom features columns
#Add label's more descriptive values as well as adding appropriate category for each symbol
#8820 training items in total, so 2940 total observations for each Category
trainfeatures["Category"] <- NA
trainfeatures$Category[1:2940] <- "Digits"
trainfeatures$Category[2941:5880] <- "Letters"
trainfeatures$Category[5881:8820] <- "Math"
trainfeatures$Category <- as.factor(trainfeatures$Category)
trainfeatures$label[trainfeatures$label==11] <- '1'
trainfeatures$label[trainfeatures$label==12] <- '2'
trainfeatures$label[trainfeatures$label==13] <- '3'
trainfeatures$label[trainfeatures$label==14] <- '4'
trainfeatures$label[trainfeatures$label==15] <- '5'
trainfeatures$label[trainfeatures$label==16] <- '6'
trainfeatures$label[trainfeatures$label==17] <- '7'
trainfeatures$label[trainfeatures$label==21] <- 'a'
trainfeatures$label[trainfeatures$label==22] <- 'b'
trainfeatures$label[trainfeatures$label==23] <- 'c'
trainfeatures$label[trainfeatures$label==24] <- 'd'
trainfeatures$label[trainfeatures$label==25] <- 'e'
trainfeatures$label[trainfeatures$label==26] <- 'f'
trainfeatures$label[trainfeatures$label==27] <- 'g'
trainfeatures$label[trainfeatures$label==31] <- '<'
trainfeatures$label[trainfeatures$label==32] <- '>'
trainfeatures$label[trainfeatures$label==33] <- '='
trainfeatures$label[trainfeatures$label==34] <- '<='
trainfeatures$label[trainfeatures$label==35] <- '>='
trainfeatures$label[trainfeatures$label==36] <- 'not='
trainfeatures$label[trainfeatures$label==37] <- 'approx'
trainfeatures$label <- as.factor(trainfeatures$label)
str(trainfeatures)
set.seed(3060)
vars <- paste(names(trainfeatures)[3:12],sep="")
tenfeatures <- paste("label ~", paste(vars, collapse="+"))
tenfeatures <- as.formula(tenfeatures)
tenfeatures
ks <- c(1,3,5,7,9,11,15,19,23,31)
set.seed(3060)
knnFit <- train(tenfeatures, data = trainfeatures, method = "knn", preProcess = c("center","scale"), tuneGrid = expand.grid(k = ks))
#Output of kNN fit
knnFit
#Output of kNN fit
knnFit$results
#Output of kNN fit
knnFit$results[, 1:2]
knnFit$results[, 1:2] # Just K values and accuracy
set.seed(3060)
ctrl <- trainControl(method="cv",number = 5, savePredictions = TRUE) #,classProbs=TRUE,summaryFunction = twoClassSummary)
knnFit1 <- train(tenfeatures, data = trainfeatures, method = "knn", preProcess = c("center","scale"),
tuneGrid = expand.grid(k = ks), trControl = ctrl)
knnFit1$results[,1:2] #Just K values and accuracies
knnFit1 #Output of kNN fit
set.seed(3060)
ctrl <- trainControl(method="cv",number = 5, savePredictions = TRUE) #,classProbs=TRUE,summaryFunction = twoClassSummary)
knnFit1 <- train(tenfeatures, data = trainfeatures, method = "knn", preProcess = c("center","scale"),
tuneGrid = expand.grid(k = ks), trControl = ctrl)
knnFit1 #Output of kNN fit
knnFit1$results[,1:2] #Just K values and accuracies
knnpredict1 <- predict(knnFit1, newdata = trainfeatures)
knnpredict1
onedivk <- 1/ks
onedivk
err_rate <- 1- knnFit$results['Accuracy']
err_rate["Cross_Validated"] <- "Classification Error Rate"
err_rate <- cbind(err_rate, onedivk)
err_rate
err_rate1 <- 1- knnFit1$results['Accuracy']
err_rate1["Cross_Validated"] <- "5-fold Crossvalidated Classification Error Rate"
err_rate1 <- cbind(err_rate1, onedivk)
err_rate1
error_rate <- rbind(err_rate, err_rate1)
error_rate
# Maybe don't make the x limit 0.3? Not sure yet review later
p<-ggplot(error_rate, aes(x=onedivk, y=Accuracy, group=Cross_Validated)) +
geom_line(aes(color=Cross_Validated))+
geom_point(aes(color=Cross_Validated)) +
labs(title="Error rate of models",x="1/K", y = "Error Rate", fill = "")+
expand_limits(x = 0, y = 0.3)+
theme(legend.position = c(0.7, 0.2))
p
matrix1 <- confusionMatrix(knnpredict1, trainfeatures$label) # Not sure if I should use testing or not, maybe should use whole dataset
matrix1 # contains accuracy on the testing data
confusionmatrix1 <- matrix1$table
confusionmatrix1 # Confusion Matrix for 5-fold CV. Convert to dataframe later
confusionmatrix1 # Confusion Matrix for 5-fold CV. Convert to dataframe later
confusionmatrix1 # Confusion Matrix for 5-fold CV. Convert to dataframe later
knnpredict1 <- predict(knnFit1$finalModel, newdata = trainfeatures)
knnpredict1 <- predict(knnFit1$finalModel, newdata = trainfeatures)
knnpredict1 <- predict(knnFit1$bestTune, newdata = trainfeatures)
knnpredict1 <- predict(knnFit1$finalModel, newdata = trainfeatures)
knnpredict1 <- predict(knnFit1$, newdata = trainfeatures)
knnpredict1
onedivk <- 1/ks
onedivk
err_rate <- 1- knnFit$results['Accuracy']
err_rate["Cross_Validated"] <- "Classification Error Rate"
err_rate <- cbind(err_rate, onedivk)
err_rate
err_rate1 <- 1- knnFit1$results['Accuracy']
err_rate1["Cross_Validated"] <- "5-fold Crossvalidated Classification Error Rate"
err_rate1 <- cbind(err_rate1, onedivk)
err_rate1
error_rate <- rbind(err_rate, err_rate1)
error_rate
# Maybe don't make the x limit 0.3? Not sure yet review later
p<-ggplot(error_rate, aes(x=onedivk, y=Accuracy, group=Cross_Validated)) +
geom_line(aes(color=Cross_Validated))+
geom_point(aes(color=Cross_Validated)) +
labs(title="Error rate of models",x="1/K", y = "Error Rate", fill = "")+
expand_limits(x = 0, y = 0.3)+
theme(legend.position = c(0.7, 0.2))
p
matrix1 <- confusionMatrix(knnpredict1, trainfeatures$label) # Not sure if I should use testing or not, maybe should use whole dataset
matrix1 # contains accuracy on the testing data
confusionmatrix1 <- matrix1$table
confusionmatrix1 # Confusion Matrix for 5-fold CV. Convert to dataframe later
# Only need digits confusion matrix
digits = c('1','2','3','4','5','6','7')
confusionmatrix1[digits,digits]
